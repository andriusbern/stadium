environment: {}

main:
    model: PPO2
    policy: CustomMlpPolicy
    n_workers: 1             # Parallel environment count
    steps_to_train: 100000         # Steps to train
    logs:
        - steps
        - goals_reached

PPO2:
    gamma: 0.98          # Discount factor for future rewards
    n_steps: 512         # Batch size (n_steps * n_workers)
    ent_coef: 0.01       # Entropy loss coefficient 
    learning_rate: 0.00025 # LR
    vf_coef: 0.5         # The contribution of value function loss to the total loss
    max_grad_norm: 0.5   # Max range of the gradient clipping
    lam: 0.95            # Generalized advantage estimation (variance/bias tradeoff)
    nminibatches: 4      # Number of minibatches for SGD/Adam updates
    noptepochs: 4        # Number of iterations for SGD/Adam
    cliprange: 0.2       # Clip factor for PPO 
#
    verbose: 2

CustomMlpPolicy:
    layers:
        - 4
        - 4

# MODELS:
# -----
# For discrete action space environments:
#     PPO2, DQN, ACER, A2C, ACKTR
# For continuous action space environments:
#     PPO2, A2C
# POLICIES:
# ---------
#     Configurable:
#         CustomCnnPolicy - CNN feature extraction
#         CustomLSTMPolicy - LSTM cells followed by a multilayer perceptron
#         CustomMlpPolicy - Multilayer perceptron
#         CustomDQNPolicy - Multilayer perceptron specifically for DQN
#     Defaults:
#         CnnPolicy - CNN as descr<Kibed in 2014 Atari paper
#         MlpPolicy - simple MLP with two hidden layers of size 64

# Arguments that get passed to the constructor of your class instance
# as config={} keyword