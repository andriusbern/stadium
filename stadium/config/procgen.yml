
environment: {}


main:
    model: PPO2
    policy: CustomCnnPolicy
    n_workers: 1             # Parallel environment count
    steps_to_train: 100000         # Steps to train
    logs:
        - steps
        - goals_reached

PPO2:
    gamma: 0.995          # Discount factor for future rewards
    n_steps: 256         # Batch size (n_steps * n_workers)
    ent_coef: 0.01       # Entropy loss coefficient 
    learning_rate: 0.0005 # LR
    vf_coef: 0.5         # The contribution of value function loss to the total loss
    max_grad_norm: 0.5   # Max range of the gradient clipping
    lam: 0.95            # Generalized advantage estimation (variance/bias tradeoff)
    nminibatches: 4      # Number of minibatches for SGD/Adam updates
    noptepochs: 4        # Number of iterations for SGD/Adam
    cliprange: 0.2       # Clip factor for PPO 
    verbose: 2

CustomMlpPolicy:
    layers:
        - 64
        - 32
    # h_actor: # Policy head
        # - 8
    # h_critic: # Value head
        # - 8

CustomCnnPolicy:
    filters:
        - 32
        - 16
        - 16
        - 8
    kernel_size:
        - 3
        - 3
        - 3
        - 3
    stride:
        - 1
        - 2
        - 2
        - 2
    layers:      # Number of nodes in the layers of the shared part of the fully connected network
        - 32
        - 16
    # h_actor: []  # Number of nodes per layer in the actor part of the network
    # h_critic: [] # Number of nodes per layer in the critic part of the network

    activ: relu
    pd_init_scale: 0.05
    conv_init_scale: 1.4
    kernel_initializer: glorot_normal_initializer
    init_bias: .5
